---
layout: best-practice
title: "Glossary"
order: 710
icon: /assets/climate-icons/Icon-Briefcase.svg

section: Conclusion and Glossary
chapter-tag: use-your-influence

previous-page: conclusion
---

# Glossary, Definitions and Terminology

| **Term** | **Definition** |
|---|---|
| <a id="bias-general"></a>**Bias (General)** | Systematic errors in AI systems that can lead to unfair or discriminatory outcomes, often reflecting societal biases in training data or algorithm design. |
| <a id="bias-neural-network"></a>**Bias (Neural Network)** | A bias is one of the key internal parameters of neural networks, along with weights. It allows the neuron to be more or less activated for a given set of inputs and weights. |
| <a id="carbon-free-energy-cfe"></a>**Carbon Free Energy (CFE)** | Percentage of renewable energy used as a proportion of the total energy used. |
| <a id="carbon-intensity"></a>**Carbon Intensity** | Carbon intensity is a measure of how clean our electricity is. It refers to how many grams of carbon dioxide (CO2) are released to produce a kilowatt hour (kWh) of electricity. |
| <a id="cnn-convolutional-neural-network"></a>**CNN (Convolutional Neural Network)** | A type of Neural Network primarily used for image recognition and processing. |
| <a id="co2e"></a>**CO2e** | Carbon Dioxide Equivalent - used to account for other GHG like Methane, NO2... |
| <a id="compression"></a>**Compression** | Compression in machine learning is a set of techniques that reduce the size and computational complexity of a model while maintaining or improving its performance. This is done to make the model smaller and more efficient. |
| <a id="context-window"></a>**Context Window** | In LLMs, the number of input tokens a model can manage at once, during inference. This includes user input, files, and conversation history. |
| <a id="csrd"></a>**CSRD** | Corporate and Sustainability Reporting Directive. EU rule mandating carbon emissions reporting for large (over 1000 employees) companies doing business in Europe. |
| <a id="cv-computer-vision"></a>**CV (Computer Vision)** | A field of AI that trains computers to interpret and understand the visual world |
| <a id="demand-shaping"></a>**Demand shaping** | General definition of demand shaping is inherited from supply chain strategies that involve influencing customer demand for a product to match a company's supply. In the context of sustainability it is changing the behavior of your product depending on the carbon intensity of your grid, - e.g. maybe you serve less computationally intense recommendations when the user’s grid is dirty. |
| <a id="distillation"></a>**Distillation** | Distillation is a technique that focuses on transferring knowledge between a large model known as the “teacher” into a smaller, more efficient model, known as the “student”. By sharing soft labels or intermediate representations, the smaller models become faster, cheaper, and more energy-efficient. |
| <a id="dl-deep-learning"></a>**DL (Deep Learning)** | A subset of ML that uses artificial neural networks with multiple layers to learn from large amounts of data. ML learning using multi-layered Neural Networks |
| <a id="emboddied-emissions"></a>**Emboddied Emissions** | The amount of carbon pollution emitted during the creation and disposal of a device |
| <a id="ai-edge-computing"></a>**AI Edge computing** | AI edge computing, also known as "AI on the edge", is the combination of artificial intelligence (AI) and edge computing to process data locally on devices. This allows for faster processing, reduced latency, and improved privacy |
| <a id="federated-learning"></a>**Federated Learning** | Federated learning aims at training a machine learning algorithm, for instance deep neural networks, on multiple local datasets contained in local nodes without explicitly exchanging data samples. Each node builds its own small model using its local data and periodically exchanges model parameters (like weights and biases) to be combined into a shared global model accessible to all nodes. In other words it is a distributed learning paradigm that prioritizes data privacy by keeping data local on each device. This limits model training and reduces processing time. |
| <a id="fine-tuning"></a>**Fine-Tuning** | The process to adjusting model parameters to improve performance |
| <a id="frugal-ai-systems"></a>**Frugal AI systems** | AI systems designed to reduce the overall need for material and energy resources, and the associated environmental impacts, by redefining usage or performance requirements, or by redirecting needs (General Framework for Frugal AI, 2024) |
| <a id="gen-ai-generative-ai"></a>**Gen AI (Generative AI)** | AI systems that can create new content, including text, images, code, and more, based on their training data |
| <a id="ghg"></a>**GHG** | GreenHouse Gas |
| <a id="greenhouse-gas-protocol"></a>**GreenHouse Gas protocol** | The most widely used and internationally recognized greenhouse gas accounting standard. Breaks down emissions into 3 main categories: scope 1, 2 and 3. |
| <a id="gpu-graphics-processing-unit"></a>**GPU (Graphics Processing Unit)** | A specialized processor originally designed for computer graphics, now widely used in AI for parallel processing |
| <a id="green-it"></a>**Green IT** | **“Green IT”** refers to the design, use, disposal, and recycling of information technology products and systems in an environmentally responsible manner. Sustainable Cloud means the consumption of cloud services in a sustainable way, to minimize their environmental impact. It includes: <br>• Data center infrastructure and operation <br>• Digital services running on this infrastructure <br>• How these services are developed, designed, deployed and operated. |
| <a id="hallucinations"></a>**Hallucinations** | When an AI model generates output that is fluent and plausible-sounding but factually incorrect or nonsensical |
| <a id="iac"></a>**IaC** | Infrastructure as Code |
| <a id="inference"></a>**Inference** | AI inference is a phase in the AI model lifecycle that follows the AI training phase. It is the process by which a trained model applies learned patterns to new, unseen data to generate predictions, insights, or decisions. It enables AI to reason and draw conclusions beyond its training data, mimicking human-like pattern recognition and decision-making. |
| <a id="llm"></a>**LLM** | Large language models are a type of AI algorithm that uses deep learning techniques and massively large data sets to understand, summarize, generate and predict new content. Theses models dramatically increased  the amount of data used for training and inference compared to previous language models (definition: from: “building green software” book) |
| <a id="ml-machine-learning"></a>**ML (Machine Learning)** | A subset of AI that focuses on algorithms that can learn from and make predictions or decisions based on data |
| <a id="model-training"></a>**Model Training** | Feeding a Machine Learning Algorithm with data and learn the optimal attributes of that data based on a desired outcome |
| <a id="nlp-natural-language-processing"></a>**NLP (Natural Language Processing)** | A branch of AI that deals with the interaction between computers and humans using natural language |
| <a id="nn-neural-network"></a>**NN (Neural Network)** | A computing system inspired by biological neural networks in the brain |
| <a id="okr"></a>**OKR** | Objective and Key Results. Framework for businesses to execute and achieve their desired strategies through simple, collaborative goal setting. |
| <a id="prompt-compression"></a>**Prompt Compression** | Prompt compression is the process of reducing the length of an input prompt while retaining the essential information needed for a language model to understand and generate a relevant response. |
| <a id="pruning"></a>**Pruning** | Pruning is a technique used to reduce the size of a deep learning model. It can improve model efficiency and speed up inference. Pruning a network can be thought of as removing unused parameters from the over parameterized network. Pruning is the process of removing weight connections in a network to increase inference speed and decrease model storage size, which then provides the ability to deploy significantly smaller and faster models, all while minimally affecting (and in some cases improving) metrics such as accuracy. |
| <a id="pue"></a>**PUE** | Power Usage Efficiency |
| <a id="quantization"></a>**Quantization** | Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer instead of the usual 32-bit floating point. Reducing the number of bits means the resulting model requires less memory storage, consumes less energy (in theory), and operations like matrix multiplication can be performed much faster |
| <a id="rag-retrieval-augmented-generation"></a>**RAG (Retrieval-Augmented Generation)** | An AI technique that enhances language models by retrieving relevant information from a knowledge base to generate more accurate and contextual responses |
| <a id="rl-reinforcement-learning"></a>**RL (Reinforcement Learning)** | An area of ML where an agent learns to make decisions by interacting with an environment |
| <a id="rnn-recurrent-neural-network"></a>**RNN (Recurrent Neural Network)** | A type of NN where connections between nodes form a directed graph along a temporal sequence, used for tasks like speech recognition |
| <a id="saas"></a>**SaaS** | Software as a Service |
| <a id="sbti"></a>**SBTi** | Science Based Target Initiative. A body that defines and promotes best practice in science-based target setting. For example, creating the standards for net zero. |
| <a id="sci"></a>**SCI** | Software Carbon Intensity*. SCI is the ratio of total emissions with a functional unit representative of the software itself. It allows to normalize and more easily compare similar software. It also includes embodied emissions and does not include carbon offsets. **SCI = (E * I) + M / R** <br>• **E** is the **Energy consumption** of the software component <br>• **I** is the **carbon Intensity factor** of the ****grid powering the data center (or cloud provider region) <br>• **M** is the **embodied emissions** of the hardware running your AI program: manufacturing and end of life. M is time-share and resource-share, which means you only account for the time your software is running on the server, and the % of resources it is consuming on the server. <br>• **R** is the **Functional Unit**, matching how the software scales. for instance, number of users, number of API calls, number of inferences <br>• More information on the SCI is available [here](https://sci.greensoftware.foundation/). |
| <a id="sufficiency"></a>**Sufficiency** | set of measures and daily practices that avoid demand for energy, materials, land and water while delivering human well-being for all within planetary boundaries (IPCC, Mitigation of Climate Change, 2022) |
| <a id="transfer-learning"></a>**Transfer learning** | Transfer learning (TL) is a ML technique where a model pre-trained on one task is fine-tuned for a new, related task saving incredible amounts of process and computing. Training a new ML model is a time-consuming and intensive process that requires a large amount of data, computing power, and several iterations before it is ready for production. |
| <a id="token-and-tokenization"></a>**Token and Tokenization** | Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. |
| <a id="tpu-tensor-processing-unit"></a>**TPU (Tensor Processing Unit)** | An AI accelerator ASIC (Application-Specific Integrated Circuit) developed by Google for neural network ML. |
| <a id="transformer"></a>**Transformer** | A type of neural network architecture that uses self-attention mechanisms, enabling it to process sequential data (like language) very effectively. It forms the basis of many modern NLP models, including LLMs |
| <a id="weights"></a>**Weights** | Numerical parameters of connections in a neural network. |
| <a id="wue"></a>**WUE** | Water Usage Efficiency |

Source: [Pascal Joly](https://maven.com/it-climate-ed/sustainable-ai/), Hugging Face, AWS, NLP Stanford, [Green Software Foundation](https://learn.greensoftware.foundation/glossary/), NationalGrid (and others)